# Image Captioning with Transformers: Transforming Visual Content into Audio for the Visually Impaired

## Project Overview

This project, titled "Image Captioning with Transformers," was developed as the final coursework for the subject Modern Analytics. The primary focus was on leveraging Convolutional Neural Networks (CNN) in conjunction with transformers to facilitate image captioning. CNNs were employed to process image features, while transformers were utilized for generating descriptive captions.

## Technologies and Tools Used

- **Programming Language:** Python
- **Deep Learning Framework:** PyTorch
- **Convolutional Neural Network (CNN):** ResNet

## Objectives

The primary objective of this project is to contribute to the independence of visually impaired individuals by transforming visual content into audio descriptions. While this project represents an initial step, the overarching goal is to provide a meaningful impact on the lives of the visually impaired.

## Learning Outcomes

1. **Positional Embedding:** Understanding and implementation of positional embedding techniques in the context of image captioning.
2. **Natural Language Processing (NLP):** Exploration and application of NLP techniques for generating human-like captions from visual content.
3. **Transformers and Autoencoders:** Proficiency in working with transformers and gaining insights into their application in conjunction with autoencoders.

## Acknowledgements

This project is a result of dedicated coursework in Modern Analytics, and the development was guided by a commitment to improving accessibility and inclusivity for individuals with visual impairments.

## Future Enhancements

While this project represents a significant achievement, future enhancements could include refining the model, expanding the dataset for improved generalization, and exploring additional technologies to further enhance the user experience.


## License

This project is licensed under the [MIT License](LICENSE).
